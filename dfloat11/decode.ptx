//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-36424714
// Cuda compilation tools, release 13.0, V13.0.88
// Based on NVVM 7.0.1
//

.version 9.0
.target sm_75
.address_size 64

	// .globl	decode
.extern .shared .align 16 .b8 shared_mem[];

.visible .entry decode(
	.param .u64 decode_param_0,
	.param .u64 decode_param_1,
	.param .u64 decode_param_2,
	.param .u64 decode_param_3,
	.param .u64 decode_param_4,
	.param .u64 decode_param_5,
	.param .u32 decode_param_6,
	.param .u32 decode_param_7,
	.param .u32 decode_param_8
)
{
	.reg .pred 	%p<56>;
	.reg .b16 	%rs<112>;
	.reg .b32 	%r<358>;
	.reg .b64 	%rd<236>;


	ld.param.u64 	%rd44, [decode_param_0];
	ld.param.u64 	%rd49, [decode_param_1];
	ld.param.u64 	%rd50, [decode_param_2];
	ld.param.u64 	%rd45, [decode_param_3];
	ld.param.u64 	%rd46, [decode_param_4];
	ld.param.u64 	%rd47, [decode_param_5];
	ld.param.u32 	%r79, [decode_param_6];
	ld.param.u32 	%r80, [decode_param_7];
	ld.param.u32 	%r81, [decode_param_8];
	cvta.to.global.u64 	%rd1, %rd50;
	cvta.to.global.u64 	%rd2, %rd49;
	mov.u32 	%r1, %ntid.x;
	shl.b32 	%r2, %r1, 2;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r357, %tid.x;
	mad.lo.s32 	%r5, %r3, %r1, %r357;
	shl.b32 	%r6, %r5, 3;
	setp.ge.s32 	%p1, %r6, %r80;
	mov.u64 	%rd222, 0;
	mov.u64 	%rd221, %rd222;
	@%p1 bra 	$L__BB0_2;

	cvt.s64.s32 	%rd51, %r6;
	add.s64 	%rd52, %rd2, %rd51;
	ld.global.nc.u8 	%rs29, [%rd52];
	cvt.u64.u16 	%rd53, %rs29;
	shl.b64 	%rd221, %rd53, 56;

$L__BB0_2:
	add.s32 	%r82, %r6, 1;
	setp.ge.s32 	%p2, %r82, %r80;
	cvt.s64.s32 	%rd55, %r6;
	add.s64 	%rd5, %rd2, %rd55;
	@%p2 bra 	$L__BB0_4;

	ld.global.nc.u8 	%rs30, [%rd5+1];
	cvt.u64.u16 	%rd56, %rs30;
	shl.b64 	%rd57, %rd56, 48;
	and.b64  	%rd222, %rd57, 71776119061217280;

$L__BB0_4:
	add.s32 	%r83, %r6, 2;
	setp.ge.s32 	%p3, %r83, %r80;
	mov.u64 	%rd224, 0;
	mov.u64 	%rd223, %rd224;
	@%p3 bra 	$L__BB0_6;

	ld.global.nc.u8 	%rs31, [%rd5+2];
	cvt.u64.u16 	%rd59, %rs31;
	shl.b64 	%rd60, %rd59, 40;
	and.b64  	%rd223, %rd60, 280375465082880;

$L__BB0_6:
	add.s32 	%r84, %r6, 3;
	setp.ge.s32 	%p4, %r84, %r80;
	@%p4 bra 	$L__BB0_8;

	ld.global.nc.u8 	%rs32, [%rd5+3];
	cvt.u64.u16 	%rd62, %rs32;
	shl.b64 	%rd63, %rd62, 32;
	and.b64  	%rd224, %rd63, 1095216660480;

$L__BB0_8:
	add.s32 	%r85, %r6, 4;
	setp.ge.s32 	%p5, %r85, %r80;
	mov.u64 	%rd226, 0;
	mov.u64 	%rd225, %rd226;
	@%p5 bra 	$L__BB0_10;

	ld.global.nc.u8 	%rs33, [%rd5+4];
	cvt.u32.u16 	%r86, %rs33;
	and.b32  	%r87, %r86, 255;
	mul.wide.u32 	%rd225, %r87, 16777216;

$L__BB0_10:
	add.s32 	%r88, %r6, 5;
	setp.ge.s32 	%p6, %r88, %r80;
	@%p6 bra 	$L__BB0_12;

	ld.global.nc.u8 	%rs34, [%rd5+5];
	cvt.u32.u16 	%r89, %rs34;
	and.b32  	%r90, %r89, 255;
	mul.wide.u32 	%rd226, %r90, 65536;

$L__BB0_12:
	add.s32 	%r91, %r6, 6;
	setp.ge.s32 	%p7, %r91, %r80;
	mov.u64 	%rd228, 0;
	mov.u64 	%rd227, %rd228;
	@%p7 bra 	$L__BB0_14;

	ld.global.nc.u8 	%rs35, [%rd5+6];
	cvt.u32.u16 	%r92, %rs35;
	and.b32  	%r93, %r92, 255;
	mul.wide.u32 	%rd227, %r93, 256;

$L__BB0_14:
	add.s32 	%r94, %r6, 7;
	setp.ge.s32 	%p8, %r94, %r80;
	@%p8 bra 	$L__BB0_16;

	ld.global.nc.u8 	%rs36, [%rd5+7];
	cvt.u64.u16 	%rd68, %rs36;
	and.b64  	%rd228, %rd68, 255;

$L__BB0_16:
	add.s32 	%r96, %r6, 8;
	setp.ge.s32 	%p9, %r96, %r80;
	mov.u32 	%r339, 0;
	mov.u32 	%r338, %r339;
	@%p9 bra 	$L__BB0_18;

	ld.global.nc.u8 	%rs37, [%rd5+8];
	cvt.u32.u16 	%r97, %rs37;
	shl.b32 	%r338, %r97, 24;

$L__BB0_18:
	add.s32 	%r99, %r6, 9;
	setp.ge.s32 	%p10, %r99, %r80;
	@%p10 bra 	$L__BB0_20;

	ld.global.nc.u8 	%rs38, [%rd5+9];
	cvt.u32.u16 	%r100, %rs38;
	shl.b32 	%r101, %r100, 16;
	and.b32  	%r339, %r101, 16711680;

$L__BB0_20:
	add.s32 	%r103, %r6, 10;
	setp.ge.s32 	%p11, %r103, %r80;
	mov.u32 	%r341, 0;
	mov.u32 	%r340, %r341;
	@%p11 bra 	$L__BB0_22;

	ld.global.nc.u8 	%rs39, [%rd5+10];
	mul.wide.u16 	%r340, %rs39, 256;

$L__BB0_22:
	add.s32 	%r105, %r6, 11;
	setp.ge.s32 	%p12, %r105, %r80;
	@%p12 bra 	$L__BB0_24;

	ld.global.nc.u8 	%rs41, [%rd5+11];
	cvt.u32.u16 	%r106, %rs41;
	and.b32  	%r341, %r106, 255;

$L__BB0_24:
	mov.u32 	%r108, shared_mem;
	add.s32 	%r17, %r108, %r2;
	add.s32 	%r15, %r17, 4;
	bar.sync 	0;
	mul.lo.s32 	%r109, %r5, 5;
	shr.s32 	%r110, %r109, 31;
	shr.u32 	%r111, %r110, 29;
	add.s32 	%r112, %r109, %r111;
	shr.s32 	%r113, %r112, 3;
	cvt.s64.s32 	%rd69, %r113;
	cvta.to.global.u64 	%rd70, %rd46;
	add.s64 	%rd71, %rd70, %rd69;
	ld.global.nc.u8 	%rs42, [%rd71+1];
	cvt.u32.u16 	%r114, %rs42;
	and.b32  	%r115, %r114, 255;
	ld.global.nc.u8 	%rs43, [%rd71];
	mov.u32 	%r346, 0;
	cvt.u32.u16 	%r116, %rs43;
	prmt.b32 	%r117, %r116, %r115, 30212;
	and.b32  	%r118, %r112, -8;
	sub.s32 	%r119, %r118, %r109;
	add.s32 	%r120, %r119, 11;
	shr.u32 	%r121, %r117, %r120;
	cvt.u64.u32 	%rd72, %r121;
	cvt.u16.u32 	%rs44, %r121;
	and.b16  	%rs107, %rs44, 31;
	and.b64  	%rd73, %rd224, 1095216660480;
	or.b64  	%rd74, %rd73, %rd223;
	or.b64  	%rd75, %rd74, %rd225;
	or.b64  	%rd76, %rd75, %rd226;
	and.b64  	%rd77, %rd76, 281474976645120;
	or.b64  	%rd78, %rd77, %rd227;
	or.b64  	%rd79, %rd78, %rd228;
	or.b64  	%rd20, %rd222, %rd221;
	or.b64  	%rd80, %rd20, %rd79;
	and.b64  	%rd232, %rd72, 31;
	and.b32  	%r122, %r121, 31;
	shl.b64 	%rd229, %rd80, %r122;
	shl.b32 	%r123, %r79, 8;
	add.s32 	%r16, %r123, -256;
	cvta.to.global.u64 	%rd23, %rd47;
	cvta.to.global.u64 	%rd24, %rd45;
	mov.u16 	%rs105, %rs107;

$L__BB0_25:
	shr.u64 	%rd82, %rd229, 56;
	add.s64 	%rd81, %rd44, %rd82;
	// begin inline asm
	ld.global.nc.u8 %r343, [%rd81];
	// end inline asm
	cvt.u16.u32 	%rs45, %r343;
	and.b16  	%rs46, %rs45, 255;
	setp.lt.u16 	%p13, %rs46, 240;
	@%p13 bra 	$L__BB0_29;

	shl.b32 	%r126, %r343, 8;
	and.b32  	%r127, %r126, 65280;
	mov.u32 	%r128, 65536;
	sub.s32 	%r129, %r128, %r127;
	cvt.u64.u32 	%rd84, %r129;
	shr.u64 	%rd85, %rd229, 48;
	and.b64  	%rd86, %rd85, 255;
	or.b64  	%rd87, %rd86, %rd84;
	add.s64 	%rd83, %rd44, %rd87;
	// begin inline asm
	ld.global.nc.u8 %r343, [%rd83];
	// end inline asm
	cvt.u16.u32 	%rs47, %r343;
	and.b16  	%rs48, %rs47, 255;
	setp.lt.u16 	%p14, %rs48, 240;
	@%p14 bra 	$L__BB0_29;

	shl.b32 	%r131, %r343, 8;
	and.b32  	%r132, %r131, 65280;
	sub.s32 	%r134, %r128, %r132;
	cvt.u64.u32 	%rd89, %r134;
	shr.u64 	%rd90, %rd229, 40;
	and.b64  	%rd91, %rd90, 255;
	or.b64  	%rd92, %rd91, %rd89;
	add.s64 	%rd88, %rd44, %rd92;
	// begin inline asm
	ld.global.nc.u8 %r343, [%rd88];
	// end inline asm
	cvt.u16.u32 	%rs49, %r343;
	and.b16  	%rs50, %rs49, 255;
	setp.lt.u16 	%p15, %rs50, 240;
	@%p15 bra 	$L__BB0_29;

	shl.b32 	%r136, %r343, 8;
	and.b32  	%r137, %r136, 65280;
	mov.u32 	%r138, 65536;
	sub.s32 	%r139, %r138, %r137;
	cvt.u64.u32 	%rd94, %r139;
	shr.u64 	%rd95, %rd229, 32;
	and.b64  	%rd96, %rd95, 255;
	or.b64  	%rd97, %rd96, %rd94;
	add.s64 	%rd93, %rd44, %rd97;
	// begin inline asm
	ld.global.nc.u8 %r343, [%rd93];
	// end inline asm

$L__BB0_29:
	add.s32 	%r346, %r346, 1;
	and.b32  	%r141, %r343, 255;
	add.s32 	%r142, %r16, %r141;
	cvt.s64.s32 	%rd99, %r142;
	add.s64 	%rd98, %rd44, %rd99;
	// begin inline asm
	ld.global.nc.u8 %r140, [%rd98];
	// end inline asm
	cvt.u16.u32 	%rs51, %r140;
	and.b32  	%r143, %r140, 255;
	shl.b64 	%rd229, %rd229, %r143;
	add.s16 	%rs105, %rs105, %rs51;
	and.b16  	%rs52, %rs105, 255;
	setp.lt.u16 	%p16, %rs52, 32;
	@%p16 bra 	$L__BB0_25;

	or.b32  	%r144, %r339, %r338;
	or.b32  	%r25, %r144, %r340;
	or.b32  	%r356, %r25, %r341;
	cvt.u64.u32 	%rd27, %r356;
	cvt.u64.u16 	%rd100, %rs105;
	and.b64  	%rd101, %rd100, 255;
	add.s64 	%rd102, %rd101, 4294967264;
	cvt.u32.u64 	%r145, %rd102;
	shl.b64 	%rd103, %rd27, %r145;
	or.b64  	%rd230, %rd103, %rd229;
	add.s16 	%rs106, %rs105, -32;
	and.b16  	%rs53, %rs106, 248;
	shr.u16 	%rs54, %rs53, 3;
	add.s16 	%rs55, %rs54, 4;
	setp.gt.u16 	%p17, %rs55, 7;
	@%p17 bra 	$L__BB0_36;

$L__BB0_31:
	shr.u64 	%rd105, %rd230, 56;
	add.s64 	%rd104, %rd44, %rd105;
	// begin inline asm
	ld.global.nc.u8 %r345, [%rd104];
	// end inline asm
	cvt.u16.u32 	%rs56, %r345;
	and.b16  	%rs57, %rs56, 255;
	setp.lt.u16 	%p18, %rs57, 240;
	@%p18 bra 	$L__BB0_35;

	shl.b32 	%r148, %r345, 8;
	and.b32  	%r149, %r148, 65280;
	mov.u32 	%r150, 65536;
	sub.s32 	%r151, %r150, %r149;
	cvt.u64.u32 	%rd107, %r151;
	shr.u64 	%rd108, %rd230, 48;
	and.b64  	%rd109, %rd108, 255;
	or.b64  	%rd110, %rd109, %rd107;
	add.s64 	%rd106, %rd44, %rd110;
	// begin inline asm
	ld.global.nc.u8 %r345, [%rd106];
	// end inline asm
	cvt.u16.u32 	%rs58, %r345;
	and.b16  	%rs59, %rs58, 255;
	setp.lt.u16 	%p19, %rs59, 240;
	@%p19 bra 	$L__BB0_35;

	shl.b32 	%r153, %r345, 8;
	and.b32  	%r154, %r153, 65280;
	sub.s32 	%r156, %r150, %r154;
	cvt.u64.u32 	%rd112, %r156;
	shr.u64 	%rd113, %rd230, 40;
	and.b64  	%rd114, %rd113, 255;
	or.b64  	%rd115, %rd114, %rd112;
	add.s64 	%rd111, %rd44, %rd115;
	// begin inline asm
	ld.global.nc.u8 %r345, [%rd111];
	// end inline asm
	cvt.u16.u32 	%rs60, %r345;
	and.b16  	%rs61, %rs60, 255;
	setp.lt.u16 	%p20, %rs61, 240;
	@%p20 bra 	$L__BB0_35;

	shl.b32 	%r158, %r345, 8;
	and.b32  	%r159, %r158, 65280;
	mov.u32 	%r160, 65536;
	sub.s32 	%r161, %r160, %r159;
	cvt.u64.u32 	%rd117, %r161;
	shr.u64 	%rd118, %rd230, 32;
	and.b64  	%rd119, %rd118, 255;
	or.b64  	%rd120, %rd119, %rd117;
	add.s64 	%rd116, %rd44, %rd120;
	// begin inline asm
	ld.global.nc.u8 %r345, [%rd116];
	// end inline asm

$L__BB0_35:
	add.s32 	%r346, %r346, 1;
	and.b32  	%r163, %r345, 255;
	add.s32 	%r164, %r16, %r163;
	cvt.s64.s32 	%rd122, %r164;
	add.s64 	%rd121, %rd44, %rd122;
	// begin inline asm
	ld.global.nc.u8 %r162, [%rd121];
	// end inline asm
	cvt.u16.u32 	%rs62, %r162;
	and.b32  	%r165, %r162, 255;
	shl.b64 	%rd230, %rd230, %r165;
	add.s16 	%rs106, %rs106, %rs62;
	and.b16  	%rs63, %rs106, 248;
	shr.u16 	%rs64, %rs63, 3;
	add.s16 	%rs65, %rs64, 4;
	setp.lt.u16 	%p21, %rs65, 8;
	@%p21 bra 	$L__BB0_31;

$L__BB0_36:
	shl.b32 	%r166, %r357, 2;
	add.s32 	%r35, %r108, %r166;
	mul.wide.u32 	%rd123, %r3, 4;
	add.s64 	%rd31, %rd24, %rd123;
	setp.eq.s32 	%p22, %r357, 0;
	@%p22 bra 	$L__BB0_38;

	st.volatile.shared.u32 	[%r35], %r346;
	bra.uni 	$L__BB0_39;

$L__BB0_38:
	ld.global.nc.u32 	%r168, [%rd31];
	add.s32 	%r169, %r168, %r346;
	st.volatile.shared.u32 	[shared_mem], %r169;

$L__BB0_39:
	bar.sync 	0;
	setp.lt.u32 	%p23, %r1, 2;
	@%p23 bra 	$L__BB0_44;

	add.s32 	%r36, %r357, 1;
	mov.u32 	%r347, 2;

$L__BB0_41:
	add.s32 	%r171, %r347, -1;
	and.b32  	%r172, %r171, %r36;
	setp.ne.s32 	%p24, %r172, 0;
	@%p24 bra 	$L__BB0_43;

	shr.u32 	%r173, %r347, 1;
	sub.s32 	%r174, %r357, %r173;
	shl.b32 	%r175, %r174, 2;
	add.s32 	%r177, %r108, %r175;
	ld.volatile.shared.u32 	%r178, [%r35];
	ld.volatile.shared.u32 	%r179, [%r177];
	add.s32 	%r180, %r178, %r179;
	st.volatile.shared.u32 	[%r35], %r180;

$L__BB0_43:
	bar.sync 	0;
	shl.b32 	%r347, %r347, 1;
	setp.le.u32 	%p25, %r347, %r1;
	@%p25 bra 	$L__BB0_41;

$L__BB0_44:
	setp.ne.s32 	%p26, %r357, 0;
	@%p26 bra 	$L__BB0_46;

	mov.u32 	%r183, 0;
	st.volatile.shared.u32 	[%r17+-4], %r183;

$L__BB0_46:
	bar.sync 	0;
	setp.lt.s32 	%p27, %r1, 2;
	@%p27 bra 	$L__BB0_51;

	add.s32 	%r40, %r357, 1;
	mov.u32 	%r348, %r1;

$L__BB0_48:
	add.s32 	%r184, %r348, -1;
	and.b32  	%r185, %r184, %r40;
	setp.eq.s32 	%p28, %r185, 0;
	@%p28 bra 	$L__BB0_49;
	bra.uni 	$L__BB0_50;

$L__BB0_49:
	shr.u32 	%r186, %r348, 1;
	sub.s32 	%r187, %r357, %r186;
	shl.b32 	%r188, %r187, 2;
	add.s32 	%r190, %r108, %r188;
	ld.volatile.shared.u32 	%r191, [%r35];
	ld.volatile.shared.u32 	%r192, [%r190];
	add.s32 	%r193, %r191, %r192;
	st.volatile.shared.u32 	[%r35], %r193;
	ld.volatile.shared.u32 	%r194, [%r190];
	ld.volatile.shared.u32 	%r195, [%r35];
	sub.s32 	%r196, %r195, %r194;
	st.volatile.shared.u32 	[%r190], %r196;

$L__BB0_50:
	shr.u32 	%r42, %r348, 1;
	bar.sync 	0;
	setp.gt.u32 	%p29, %r348, 3;
	mov.u32 	%r348, %r42;
	@%p29 bra 	$L__BB0_48;

$L__BB0_51:
	@%p26 bra 	$L__BB0_53;

	ld.global.nc.u32 	%r197, [%rd31];
	st.volatile.shared.u32 	[shared_mem], %r197;
	ld.global.nc.u32 	%r198, [%rd31+4];
	st.volatile.shared.u32 	[%r17], %r198;

$L__BB0_53:
	bar.sync 	0;
	ld.volatile.shared.u32 	%r43, [shared_mem];
	ld.volatile.shared.u32 	%r349, [%r35];
	add.s32 	%r199, %r349, %r346;
	min.u32 	%r45, %r199, %r81;
	or.b64  	%rd124, %rd225, %rd224;
	or.b64  	%rd125, %rd124, %rd226;
	or.b64  	%rd126, %rd125, %rd227;
	and.b64  	%rd127, %rd126, 1099511627520;
	or.b64  	%rd128, %rd127, %rd228;
	and.b64  	%rd129, %rd223, 280375465082880;
	or.b64  	%rd130, %rd20, %rd129;
	or.b64  	%rd131, %rd130, %rd128;
	cvt.u32.u64 	%r200, %rd232;
	shl.b64 	%rd231, %rd131, %r200;
	setp.ge.u32 	%p31, %r349, %r45;
	@%p31 bra 	$L__BB0_61;

	mov.u32 	%r350, %r356;

$L__BB0_55:
	shr.u64 	%rd133, %rd231, 56;
	add.s64 	%rd132, %rd44, %rd133;
	// begin inline asm
	ld.global.nc.u8 %r201, [%rd132];
	// end inline asm
	cvt.u16.u32 	%rs108, %r201;
	and.b16  	%rs66, %rs108, 255;
	setp.lt.u16 	%p32, %rs66, 240;
	@%p32 bra 	$L__BB0_59;

	shl.b32 	%r203, %r201, 8;
	and.b32  	%r204, %r203, 65280;
	mov.u32 	%r205, 65536;
	sub.s32 	%r206, %r205, %r204;
	cvt.u64.u32 	%rd135, %r206;
	shr.u64 	%rd136, %rd231, 48;
	and.b64  	%rd137, %rd136, 255;
	or.b64  	%rd138, %rd137, %rd135;
	add.s64 	%rd134, %rd44, %rd138;
	// begin inline asm
	ld.global.nc.u8 %r202, [%rd134];
	// end inline asm
	cvt.u16.u32 	%rs108, %r202;
	and.b16  	%rs67, %rs108, 255;
	setp.lt.u16 	%p33, %rs67, 240;
	@%p33 bra 	$L__BB0_59;

	shl.b32 	%r208, %r202, 8;
	and.b32  	%r209, %r208, 65280;
	sub.s32 	%r211, %r205, %r209;
	cvt.u64.u32 	%rd140, %r211;
	shr.u64 	%rd141, %rd231, 40;
	and.b64  	%rd142, %rd141, 255;
	or.b64  	%rd143, %rd142, %rd140;
	add.s64 	%rd139, %rd44, %rd143;
	// begin inline asm
	ld.global.nc.u8 %r207, [%rd139];
	// end inline asm
	cvt.u16.u32 	%rs108, %r207;
	and.b16  	%rs68, %rs108, 255;
	setp.lt.u16 	%p34, %rs68, 240;
	@%p34 bra 	$L__BB0_59;

	shl.b32 	%r213, %r207, 8;
	and.b32  	%r214, %r213, 65280;
	mov.u32 	%r215, 65536;
	sub.s32 	%r216, %r215, %r214;
	cvt.u64.u32 	%rd145, %r216;
	shr.u64 	%rd146, %rd231, 32;
	and.b64  	%rd147, %rd146, 255;
	or.b64  	%rd148, %rd147, %rd145;
	add.s64 	%rd144, %rd44, %rd148;
	// begin inline asm
	ld.global.nc.u8 %r212, [%rd144];
	// end inline asm
	cvt.u16.u32 	%rs108, %r212;

$L__BB0_59:
	cvt.u64.u32 	%rd150, %r349;
	add.s64 	%rd151, %rd1, %rd150;
	ld.global.nc.u8 	%rs69, [%rd151];
	and.b16  	%rs70, %rs69, 128;
	and.b16  	%rs71, %rs108, 254;
	shr.u16 	%rs72, %rs71, 1;
	or.b16  	%rs73, %rs70, %rs72;
	mul.wide.u16 	%r218, %rs73, 256;
	and.b16  	%rs74, %rs69, 127;
	cvt.u32.u16 	%r219, %rs74;
	cvt.u32.u16 	%r220, %rs108;
	bfi.b32 	%r221, %r220, %r219, 7, 9;
	and.b32  	%r222, %r221, 255;
	and.b32  	%r223, %r350, -65536;
	or.b32  	%r224, %r223, %r222;
	or.b32  	%r350, %r224, %r218;
	sub.s32 	%r225, %r349, %r43;
	shl.b32 	%r226, %r225, 1;
	add.s32 	%r227, %r15, %r226;
	st.volatile.shared.u16 	[%r227], %r350;
	and.b32  	%r228, %r220, 255;
	add.s32 	%r229, %r16, %r228;
	cvt.s64.s32 	%rd152, %r229;
	add.s64 	%rd149, %rd44, %rd152;
	// begin inline asm
	ld.global.nc.u8 %r217, [%rd149];
	// end inline asm
	cvt.u16.u32 	%rs75, %r217;
	and.b32  	%r230, %r217, 255;
	shl.b64 	%rd231, %rd231, %r230;
	add.s16 	%rs107, %rs107, %rs75;
	and.b16  	%rs76, %rs107, 255;
	setp.lt.u16 	%p35, %rs76, 32;
	add.s32 	%r349, %r349, 1;
	setp.lt.u32 	%p36, %r349, %r45;
	and.pred  	%p37, %p35, %p36;
	@%p37 bra 	$L__BB0_55;

	cvt.u64.u16 	%rd153, %rs107;
	and.b64  	%rd232, %rd153, 255;

$L__BB0_61:
	add.s64 	%rd154, %rd232, 4294967264;
	cvt.u32.u64 	%r231, %rd154;
	shl.b64 	%rd155, %rd27, %r231;
	or.b64  	%rd235, %rd155, %rd231;
	setp.le.u32 	%p38, %r45, %r349;
	@%p38 bra 	$L__BB0_79;

	sub.s32 	%r232, %r45, %r349;
	and.b32  	%r233, %r232, 1;
	setp.eq.b32 	%p39, %r233, 1;
	mov.pred 	%p40, 0;
	xor.pred  	%p41, %p39, %p40;
	not.pred 	%p42, %p41;
	mov.u32 	%r355, %r349;
	@%p42 bra 	$L__BB0_68;

	shr.u64 	%rd157, %rd235, 56;
	add.s64 	%rd156, %rd44, %rd157;
	// begin inline asm
	ld.global.nc.u8 %r234, [%rd156];
	// end inline asm
	cvt.u16.u32 	%rs109, %r234;
	and.b16  	%rs77, %rs109, 255;
	setp.lt.u16 	%p43, %rs77, 240;
	@%p43 bra 	$L__BB0_67;

	shl.b32 	%r236, %r234, 8;
	and.b32  	%r237, %r236, 65280;
	mov.u32 	%r238, 65536;
	sub.s32 	%r239, %r238, %r237;
	cvt.u64.u32 	%rd159, %r239;
	shr.u64 	%rd160, %rd235, 48;
	and.b64  	%rd161, %rd160, 255;
	or.b64  	%rd162, %rd161, %rd159;
	add.s64 	%rd158, %rd44, %rd162;
	// begin inline asm
	ld.global.nc.u8 %r235, [%rd158];
	// end inline asm
	cvt.u16.u32 	%rs109, %r235;
	and.b16  	%rs78, %rs109, 255;
	setp.lt.u16 	%p44, %rs78, 240;
	@%p44 bra 	$L__BB0_67;

	shl.b32 	%r241, %r235, 8;
	and.b32  	%r242, %r241, 65280;
	sub.s32 	%r244, %r238, %r242;
	cvt.u64.u32 	%rd164, %r244;
	shr.u64 	%rd165, %rd235, 40;
	and.b64  	%rd166, %rd165, 255;
	or.b64  	%rd167, %rd166, %rd164;
	add.s64 	%rd163, %rd44, %rd167;
	// begin inline asm
	ld.global.nc.u8 %r240, [%rd163];
	// end inline asm
	cvt.u16.u32 	%rs109, %r240;
	and.b16  	%rs79, %rs109, 255;
	setp.lt.u16 	%p45, %rs79, 240;
	@%p45 bra 	$L__BB0_67;

	shl.b32 	%r246, %r240, 8;
	and.b32  	%r247, %r246, 65280;
	mov.u32 	%r248, 65536;
	sub.s32 	%r249, %r248, %r247;
	cvt.u64.u32 	%rd169, %r249;
	shr.u64 	%rd170, %rd235, 32;
	and.b64  	%rd171, %rd170, 255;
	or.b64  	%rd172, %rd171, %rd169;
	add.s64 	%rd168, %rd44, %rd172;
	// begin inline asm
	ld.global.nc.u8 %r245, [%rd168];
	// end inline asm
	cvt.u16.u32 	%rs109, %r245;

$L__BB0_67:
	cvt.u64.u32 	%rd174, %r349;
	add.s64 	%rd175, %rd1, %rd174;
	ld.global.nc.u8 	%rs80, [%rd175];
	and.b16  	%rs81, %rs80, 128;
	and.b16  	%rs82, %rs109, 254;
	shr.u16 	%rs83, %rs82, 1;
	or.b16  	%rs84, %rs81, %rs83;
	mul.wide.u16 	%r251, %rs84, 256;
	and.b16  	%rs85, %rs80, 127;
	cvt.u32.u16 	%r252, %rs85;
	cvt.u32.u16 	%r253, %rs109;
	bfi.b32 	%r254, %r253, %r252, 7, 9;
	and.b32  	%r255, %r254, 255;
	and.b32  	%r256, %r25, -65536;
	or.b32  	%r257, %r256, %r255;
	or.b32  	%r356, %r257, %r251;
	sub.s32 	%r258, %r349, %r43;
	shl.b32 	%r259, %r258, 1;
	add.s32 	%r260, %r15, %r259;
	st.volatile.shared.u16 	[%r260], %r356;
	add.s32 	%r355, %r349, 1;
	and.b32  	%r261, %r253, 255;
	add.s32 	%r262, %r16, %r261;
	cvt.s64.s32 	%rd176, %r262;
	add.s64 	%rd173, %rd44, %rd176;
	// begin inline asm
	ld.global.nc.u8 %r250, [%rd173];
	// end inline asm
	and.b32  	%r263, %r250, 255;
	shl.b64 	%rd235, %rd235, %r263;

$L__BB0_68:
	mov.u32 	%r264, -2;
	sub.s32 	%r265, %r264, %r349;
	not.b32 	%r266, %r45;
	setp.eq.s32 	%p46, %r265, %r266;
	@%p46 bra 	$L__BB0_79;

	shl.b32 	%r268, %r355, 1;
	add.s32 	%r269, %r2, %r268;
	shl.b32 	%r270, %r43, 1;
	sub.s32 	%r271, %r269, %r270;
	add.s32 	%r273, %r108, %r271;
	add.s32 	%r61, %r273, 1;
	mov.u32 	%r354, 0;

$L__BB0_70:
	shr.u64 	%rd178, %rd235, 56;
	add.s64 	%rd177, %rd44, %rd178;
	// begin inline asm
	ld.global.nc.u8 %r274, [%rd177];
	// end inline asm
	cvt.u16.u32 	%rs110, %r274;
	and.b16  	%rs86, %rs110, 255;
	setp.lt.u16 	%p47, %rs86, 240;
	@%p47 bra 	$L__BB0_74;

	shl.b32 	%r276, %r274, 8;
	and.b32  	%r277, %r276, 65280;
	mov.u32 	%r278, 65536;
	sub.s32 	%r279, %r278, %r277;
	cvt.u64.u32 	%rd180, %r279;
	shr.u64 	%rd181, %rd235, 48;
	and.b64  	%rd182, %rd181, 255;
	or.b64  	%rd183, %rd182, %rd180;
	add.s64 	%rd179, %rd44, %rd183;
	// begin inline asm
	ld.global.nc.u8 %r275, [%rd179];
	// end inline asm
	cvt.u16.u32 	%rs110, %r275;
	and.b16  	%rs87, %rs110, 255;
	setp.lt.u16 	%p48, %rs87, 240;
	@%p48 bra 	$L__BB0_74;

	shl.b32 	%r281, %r275, 8;
	and.b32  	%r282, %r281, 65280;
	sub.s32 	%r284, %r278, %r282;
	cvt.u64.u32 	%rd185, %r284;
	shr.u64 	%rd186, %rd235, 40;
	and.b64  	%rd187, %rd186, 255;
	or.b64  	%rd188, %rd187, %rd185;
	add.s64 	%rd184, %rd44, %rd188;
	// begin inline asm
	ld.global.nc.u8 %r280, [%rd184];
	// end inline asm
	cvt.u16.u32 	%rs110, %r280;
	and.b16  	%rs88, %rs110, 255;
	setp.lt.u16 	%p49, %rs88, 240;
	@%p49 bra 	$L__BB0_74;

	shl.b32 	%r286, %r280, 8;
	and.b32  	%r287, %r286, 65280;
	mov.u32 	%r288, 65536;
	sub.s32 	%r289, %r288, %r287;
	cvt.u64.u32 	%rd190, %r289;
	shr.u64 	%rd191, %rd235, 32;
	and.b64  	%rd192, %rd191, 255;
	or.b64  	%rd193, %rd192, %rd190;
	add.s64 	%rd189, %rd44, %rd193;
	// begin inline asm
	ld.global.nc.u8 %r285, [%rd189];
	// end inline asm
	cvt.u16.u32 	%rs110, %r285;

$L__BB0_74:
	cvt.u64.u32 	%rd196, %r355;
	add.s64 	%rd197, %rd1, %rd196;
	ld.global.nc.u8 	%rs89, [%rd197];
	and.b16  	%rs90, %rs89, -128;
	and.b16  	%rs91, %rs110, 254;
	shr.u16 	%rs92, %rs91, 1;
	or.b16  	%rs93, %rs90, %rs92;
	cvt.u32.u16 	%r292, %rs93;
	and.b16  	%rs94, %rs89, 127;
	cvt.u32.u16 	%r293, %rs94;
	and.b32  	%r68, %r356, -65536;
	cvt.u32.u16 	%r294, %rs110;
	bfi.b32 	%r295, %r294, %r293, 7, 9;
	and.b32  	%r296, %r295, 255;
	prmt.b32 	%r297, %r292, %r296, 8452;
	add.s32 	%r298, %r61, %r354;
	st.volatile.shared.u16 	[%r298+3], %r297;
	and.b32  	%r299, %r294, 255;
	add.s32 	%r300, %r16, %r299;
	cvt.s64.s32 	%rd198, %r300;
	add.s64 	%rd194, %rd44, %rd198;
	// begin inline asm
	ld.global.nc.u8 %r290, [%rd194];
	// end inline asm
	and.b32  	%r301, %r290, 255;
	shl.b64 	%rd42, %rd235, %r301;
	shr.u64 	%rd199, %rd42, 56;
	add.s64 	%rd195, %rd44, %rd199;
	// begin inline asm
	ld.global.nc.u8 %r291, [%rd195];
	// end inline asm
	cvt.u16.u32 	%rs111, %r291;
	and.b16  	%rs95, %rs111, 255;
	setp.lt.u16 	%p50, %rs95, 240;
	@%p50 bra 	$L__BB0_78;

	shl.b32 	%r303, %r291, 8;
	and.b32  	%r304, %r303, 65280;
	mov.u32 	%r305, 65536;
	sub.s32 	%r306, %r305, %r304;
	cvt.u64.u32 	%rd201, %r306;
	shr.u64 	%rd202, %rd42, 48;
	and.b64  	%rd203, %rd202, 255;
	or.b64  	%rd204, %rd203, %rd201;
	add.s64 	%rd200, %rd44, %rd204;
	// begin inline asm
	ld.global.nc.u8 %r302, [%rd200];
	// end inline asm
	cvt.u16.u32 	%rs111, %r302;
	and.b16  	%rs96, %rs111, 255;
	setp.lt.u16 	%p51, %rs96, 240;
	@%p51 bra 	$L__BB0_78;

	shl.b32 	%r308, %r302, 8;
	and.b32  	%r309, %r308, 65280;
	sub.s32 	%r311, %r305, %r309;
	cvt.u64.u32 	%rd206, %r311;
	shr.u64 	%rd207, %rd42, 40;
	and.b64  	%rd208, %rd207, 255;
	or.b64  	%rd209, %rd208, %rd206;
	add.s64 	%rd205, %rd44, %rd209;
	// begin inline asm
	ld.global.nc.u8 %r307, [%rd205];
	// end inline asm
	cvt.u16.u32 	%rs111, %r307;
	and.b16  	%rs97, %rs111, 255;
	setp.lt.u16 	%p52, %rs97, 240;
	@%p52 bra 	$L__BB0_78;

	shl.b32 	%r313, %r307, 8;
	and.b32  	%r314, %r313, 65280;
	mov.u32 	%r315, 65536;
	sub.s32 	%r316, %r315, %r314;
	cvt.u64.u32 	%rd211, %r316;
	shr.u64 	%rd212, %rd42, 32;
	and.b64  	%rd213, %rd212, 255;
	or.b64  	%rd214, %rd213, %rd211;
	add.s64 	%rd210, %rd44, %rd214;
	// begin inline asm
	ld.global.nc.u8 %r312, [%rd210];
	// end inline asm
	cvt.u16.u32 	%rs111, %r312;

$L__BB0_78:
	add.s32 	%r318, %r355, 1;
	cvt.u64.u32 	%rd216, %r318;
	add.s64 	%rd217, %rd1, %rd216;
	ld.global.nc.u8 	%rs98, [%rd217];
	and.b16  	%rs99, %rs98, 128;
	and.b16  	%rs100, %rs111, 254;
	shr.u16 	%rs101, %rs100, 1;
	or.b16  	%rs102, %rs99, %rs101;
	mul.wide.u16 	%r319, %rs102, 256;
	and.b16  	%rs103, %rs98, 127;
	cvt.u32.u16 	%r320, %rs103;
	cvt.u32.u16 	%r321, %rs111;
	bfi.b32 	%r322, %r321, %r320, 7, 9;
	and.b32  	%r323, %r322, 255;
	or.b32  	%r324, %r68, %r323;
	or.b32  	%r356, %r324, %r319;
	add.s32 	%r337, %r298, 3;
	st.volatile.shared.u16 	[%r337+2], %r356;
	and.b32  	%r325, %r321, 255;
	add.s32 	%r326, %r16, %r325;
	cvt.s64.s32 	%rd218, %r326;
	add.s64 	%rd215, %rd44, %rd218;
	// begin inline asm
	ld.global.nc.u8 %r317, [%rd215];
	// end inline asm
	and.b32  	%r327, %r317, 255;
	shl.b64 	%rd235, %rd42, %r327;
	add.s32 	%r354, %r354, 4;
	add.s32 	%r355, %r355, 2;
	setp.lt.u32 	%p53, %r355, %r45;
	@%p53 bra 	$L__BB0_70;

$L__BB0_79:
	bar.sync 	0;
	ld.volatile.shared.u32 	%r328, [%r17];
	sub.s32 	%r329, %r328, %r43;
	sub.s32 	%r76, %r81, %r43;
	min.u32 	%r330, %r329, %r76;
	setp.ge.u32 	%p54, %r357, %r330;
	@%p54 bra 	$L__BB0_81;

$L__BB0_80:
	shl.b32 	%r331, %r357, 1;
	add.s32 	%r332, %r17, %r331;
	ld.volatile.shared.u16 	%rs104, [%r332+4];
	add.s32 	%r333, %r357, %r43;
	mul.wide.u32 	%rd219, %r333, 2;
	add.s64 	%rd220, %rd23, %rd219;
	st.global.u16 	[%rd220], %rs104;
	ld.volatile.shared.u32 	%r334, [%r17];
	sub.s32 	%r335, %r334, %r43;
	min.u32 	%r336, %r335, %r76;
	add.s32 	%r357, %r357, %r1;
	setp.lt.u32 	%p55, %r357, %r336;
	@%p55 bra 	$L__BB0_80;

$L__BB0_81:
	ret;

}

